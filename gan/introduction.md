# GAN
Generative Adversarial Network

## Basic Idea of GAN  
The GAN consists of two parts : 
- Generator G:  
  - vector $\rightarrow$ Generator $\rightarrow$  high dimensional vector  
    Each dimension of input vector reprsents some characteristics.

- Discriminator D:
  - vector $\rightarrow$ Discriminator $\rightarrow$ scalar  
    To determine whether the samples are taken from the distribution of real data  
    $$\mathbb{E}_{x \sim P_{data}(x)}\log(D(x)) + \mathbb{E}_{x \sim P_z(x)}\log(1- (D(z)))$$
    
    where $P_{data}(x)$ is the distribution of sample data and $P_z(x;θ)$ is the distribution of generated data 

    > Because the logarithm of $x<1$ is negative, in order to maximize the value of this item, need to make the mean $D(G(z))≈0$, so G does not deceive D.

    Larger value means real, smaller value means fake. 
    $$D(x) = 1\quad when\quad x \sim P_{data}(x)$$

Under this structure, we need to train two models at the same time, namely a generative model G that can capture the data distribution and a discriminant model D that can estimate the probability that the data comes from a real sample. The training process of the generator G is to maximize the probability of the discriminator making mistakes, that is, the discriminator mistakenly believes that the data is a real sample rather than a fake sample generated by the generator. Therefore, this structure corresponds to a minimax game between two participants. 

$$V(G,D) := \underset{G}{\min}\underset{D}{\max}V(D,G) = \mathbb{E}_{x \sim P_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim P_{z}(z)}[\log(1 - D(G(z)))]$$

Among all possible functions G and D, we can find a unique equilibrium solution, that is, G can generate the same distribution as the training sample, and the probability of Discriminator judgment is 1/2.

> Generator and discriminator have a confrontational relationship, They seem to be the relationship between predators and prey.

## The process of GAN
- Initialize generator G and discriminator D  
- In each training iteration : 
  - Step 1 : Fix generator G, and update discriminator   
    Feed randomly sampled vector into a fixed generator to generate objects.  
    Sample some examples from database of real thing to train discriminator and update parametes of discrimiantor. How to train it?   
    > Discriminator learns to assign high scores to real objects and low scores to generated objects.

  - Step 2 : Fix discriminator D and update generator G  
    Generator learns to "fool" the discriminator.  
    The generator’s output is evaluated by the discriminator, and hope the discriminator can give it a higher score.

Generator can only produce something like noise.  
Discriminator is to judge whether it's produced by genenrator or like the real thing.  
The next generator is to find a way to fool the discriminator of the first generation.  
The first generation discriminator can distinguish the difference between the output of the first generation generator and the real thing.  
Then the generator evolves, and the discriminator also evolves.

### How to implement?  
It actually connects the generator and discriminator into a huge network.

vector $\rightarrow$ NN Generator $\rightarrow$ hidden layer $\rightarrow$ Discriminator $\rightarrow$ score

There is a hidden layer inside the large network. The output of this hidden layer is target object generated by generator.  
In order to train generator, fix the last few layers of the huge network. Only train the first few layers with gradient ascent.  
The goal is to make the value of the entire network output as large as possible.

### Algorithm in Detail
- Initialize $\theta_d$ for D and $\theta_g$ for G
- In each training iteration :
  - Training Discriminator :
  - Sample m examples $\lbrace x^1,x^2,\cdots,x^m \rbrace$ from database
  - Sample m noise samples $\lbrace z^1,z^2,\cdots,z^m \rbrace$ from a distribution
  - Obtaining generated data $\lbrace \tilde{x}^1,\tilde{x}^2,\cdots,\tilde{x}^m \rbrace$, $\tilde{x}^i = G(z^i)$
  - Update discriminator parameters $\theta_d$ to maximize 
    $$\tilde{v} = \dfrac{1}{m}\sum_{i=1}^m\log D(x^i) + \dfrac{1}{m}\sum_{i=1}^m\log(1-D(\tilde{x}^i))$$
    $$\theta_d \rightarrow \theta_d + \eta \nabla \tilde{V}(\theta_d)$$
  - Training Generator : 
  - Sample m noise samples $\lbrace z^1,z^2,\cdots,z^m \rbrace$ from a distribution
  - Update generator parameters $\theta_g$ to maximize 
    $$\tilde{V} = \dfrac{1}{m}\sum_{i=1}^m\log(D(G(z^i)))$$
    $$\theta_g \rightarrow \theta_g - \eta \nabla \tilde{V}(\theta_g)$$

For D, we want to maximize the $V(G,D)$ (strong recognition ability), and for G, we want to minimize it (the generated data is close to the actual data). The entire training is an iterative process. In fact, the minimax game can be understood separately, that is, given G, first maximize $V(D,G)$ and train D, then fix D, and minimize $V(D,G)$ to obtain G. Among them, given G, maximize V(D,G) to evaluate the difference or distance between $P_G$ and $P_{data}$.

## Actual training process
According to the previous definition of the value function $V(G,D)$, we require two mathematical expectations, namely $E[log(D(x))]$ and $E[log(1-D(G(z)))]$, where $x$ obeys the real data distribution, $z$ obeys the initial distribution. But in practice, we have no way to use integrals to find these two mathematical expectations, so generally we can sample from infinite real data and generators to approximate real mathematical expectations.

Given the generator G, we want to calculate $\max V(G,D)$ to obtain the discriminator D, then we first need to sample m samples  $\lbrace x^1,x^2,\cdots,x^m\rbrace$ from $P_{data}(x)$, sample m samples $\lbrace \tilde{x}^1,\tilde{x}^2,\cdots,\tilde{x}^m\rbrace$ from the generator $P_G(x)$. Therefore, the maximum value function $V(G,D)$ can be approximated by the following expression:

Maximize
$\tilde{V} = \dfrac{1}{m}\sum_{i=1}^m\log D(x^i) + \dfrac{1}{m}\sum_{i=1}^m\log(1-D(\tilde{x}^i))$

If we need to calculate the above-mentioned maximization process, an equivalent form of training method can be used. If we have a binary classifier D (the parameter is $θ_d$), of course the classifier can be a deep neural network, then the output of the maximization process is the classifier $D(x)$. Now we take samples from $P_{data}(x)$ as positive samples, and samples from $P_G(x)$ as negative samples. At the same time, we take the function that approximates negative $V(G,D)$ as the loss function, so we will express it as a standard The training process of the binary classifier:

Minimize $L = -\dfrac{1}{m}\sum_{i=1}^m\log D(x^i) - \dfrac{1}{m}\sum_{i=1}^m\log(1-D(\tilde{x}^i))$

In practice, we must use iterative and numerical methods to achieve the minimax game process. Complete optimization D in the inner loop of training is computationally prohibitive, and limited data sets can also lead to overfitting. So we can alternate between k steps to optimize D and one step to optimize G. Then we only need to update G slowly, and D will always be near the optimal solution. This strategy is similar to the way of SML/PCD training.

In summary, we can describe the entire training process, for each iteration:
- Extract m samples from the real data distribution $P_{data}$
- Extract m noise samples from the prior distribution $P_{prior(z)}$
- Put noise samples into G to generate data $\lbrace \tilde{x}^1,\tilde{x}^2,\cdots,\tilde{x}^m\rbrace, \tilde{x}^i = G(z^i)$, and update the discriminator parameter $θ_d$ by maximizing the approximation of V, that is, maximize V, and the update iteration formula of the discriminator parameter is $\theta_d \rightarrow \theta_d + \eta \nabla \tilde{V}(\theta_d)$

The above is the process of learning the discriminator D. Because the process of learning D is the process of calculating JS divergence, and we hope to maximize the value function, this step will be repeated k times.

- Take another m noise samples $\lbrace z^1,z^2,\cdots,z^m \rbrace$ from the prior distribution $P_{prior}(z)$
- The generator parameter $θ_g$ is updated by minimizing $\tilde{V}$, and the update iteration formula of the generator parameter is $\theta_g \rightarrow \theta_g - \eta \nabla \tilde{V}(\theta_g)$

The above is the process of learning generator parameters. This process will only be performed once in an iteration, so it can avoid too many updates and increase the JS divergence.

## Structured Learning
Machine learning is to find a function $f : X \rightarrow Y$ 
- Regression : output a scalar
- Classification : output a "class" (one-hot vector)
- Structured Learning/Prediction : output a sequence, a matrix, a graph, a tree ...
  - Output Sequence task
    - Machine Translation
    - Speech Recognition
    - Chat-bot
  - Output Matrix
    - Image to Image
    - Text to Image

Output is composed of components with dependency.

### Why Structured Learning is challenging:
- One-shot /Zero-shot Learning :
  - In classification, each class has some examples.
  - In structured learning, if you consider each possible output as a "class". Since the output space is huge, most "classes" do not have any training data.
  - Machine has to create new stuff during testing. Need more intelligence
- Machine has to learn to do planning
  - Machine generates objects component-by-component, but it should have a big picture in its mind. Because the output components have dependency, they should be considered globally.

### Structured Learning Approach
GAN can regard as a solution for structured learning.
- Bottom Up + Top Down 
  - Bottom UP : Learn to generate the object at the component level like Generator.
  - Top Down : Evaluating the whole object, and find the best one like Discriminator

## Can Generator learn by itself?
Yes, it can do it, but it doesn't work well. Because generator would make some mistakes, some mistakes are seriou, while some are fine. What it misses are that the relation between the components are critical. When there's only a generator, although components are highly correlated, they can't influence each other as each neural in output layer is independent to represent its pixel. It need deep structure to catch the relation between components.

## Can Discriminator generate?
Yes, it can but it is difficult.  

- Discriminator is a function D(network, can deep)  
  - $D : X \rightarrow R$
    - Input x : an object (eg. an image)
    - Output D(x) : scalar which represents how "good" an object x is.
  - It is easier to catch the relation between the components by top-down evaluation

Suppose we already have a good discriminator D(X)  
- To genetate object $\tilde{x}$ that $\tilde{x} = \underset{x \in X}{\argmax D(x)}$, we need to enumerate all possible $x$. 
- Discriminator training needs some negative examples too. Good negative examples are critical.

How to train Discriminator?(General Algorithm)
- Given a set of positive examples, randomly generate a set of negative example.
- In each iteration
  - Learn a dicriminator D that can discriminate positive and negative examples.
  - Generate negative examples by discriminator D to require $\tilde{x}$ that $\tilde{x} = \underset{x \in X}{\argmax D(x)}$(which is difficulte)

## Generator v.s. Discriminator
|Type|Generator|Discriminator|
|--|--|--|
|Pros|Easy to generate even with deep model|Considering the big picture|
|Cons|Imitate the appearance and is hard to learn the correlation between components|Generation is not always feasible especilly when model is deep, and is hard to do negative sampling|

So we can combine Generator with Discriminator. Let Generator generate $\tilde{x}$ and Discriminator evaluate it.  
G $\rightarrow \tilde{x} = \tilde{x} \leftarrow \underset{x \in X}{\argmax}$ 

## Benefit of GAN
- From Discriminator's point of view
  - Using generator to generate negative samples more efficiently
- From Generator's point of view
  - Still generate the object component-by-component
  - But it is learned from the discriminator with global view.








